{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10- Testing and Debugging\n",
    "\n",
    "**Based on materials got from Anthony Scopatz and Capentry**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A software testing is to verify the correct behavior of a software. It is done in each step of development:\n",
    "* **Desin space**\n",
    "* **Design Algorithm**\n",
    "* **Implementation**\n",
    "* **And integration with other modules.**\n",
    "\n",
    "Unless you are perfectly accurate in writting codes and fully precise, you must verify your code in order to\n",
    "trust it enough. Testing is important because of the following questions:\n",
    "\n",
    "-  Does your code work? Always?\n",
    "-  Does it do what you think it does? \n",
    "-  Does not update impact your code?\n",
    "-  Does it continue to work after system configurations or libraries\n",
    "   are upgraded?\n",
    "-  Does it respond properly for a full range of input parameters?\n",
    "-  What's the limit on that input parameter?\n",
    "\n",
    "Verification and validation are the key processes of testing. *Verification* is the process of asking, \"Have we built the software correctly?\" and *Validation* is the process of asking, \"Have we built the right\n",
    "software?\"\n",
    "\n",
    "**WE MUST ALWAYS, EARLY AND OFTEN TEST CODES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.1. Types of Tests\n",
    "\n",
    "There exist a lot of testing techniques such as: exception, unit, white box, black box, alpha  and beta testings. \n",
    "In this section, we will be learning the built-in python method to test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.2. Elements of a Test\n",
    "\n",
    "**Behavior:** For example, you might want\n",
    "to test the mean() function.\n",
    "\n",
    "**Expected Result:** This might be a single number, a range of numbers,\n",
    "a new fully defined object, a system state, an exception, etc. When we\n",
    "run the mean() function, we expect to generate a numerical value.\n",
    "\n",
    "**Assertions:** Require that some conditional be true. If the\n",
    "conditional is false, the test fails.\n",
    "\n",
    "**Fixtures:** Sometimes you have to do some legwork to create the\n",
    "objects that are necessary to run one or many tests. These objects are\n",
    "called fixtures as they are not really part of the test themselves but\n",
    "rather involve getting the computer into the appropriate state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a code has three different information:\n",
    "* **A precondition** is something that must be true at the start of a function in order for it to work correctly.\n",
    "* **A postcondition** is something that the function guarantees is true when it finishes.\n",
    "* **An invariant** is something that is always true at a particular point inside a piece of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once testing has uncovered problems, the next step is to fix them. Debugging is identifying and removing errors from computer software.\n",
    "\n",
    "Many novices do this by making more-or-less random changes to their code until it seems to produce the right answer, but that’s very inefficient (and the result is usually only correct for the one case they’re testing).\n",
    "### 10.2.1-Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Know What It’s Supposed to Do\n",
    "\n",
    "The first step in debugging something is to know what it’s supposed to do. “My program doesn’t work” isn’t good enough: in order to diagnose and fix problems, we need to be able to tell correct output from incorrect. If we can write a test case for the failing case — i.e., if we can assert that with these inputs, the function should produce that result — then we’re ready to start debugging. If we can’t, then we need to figure out how we’re going to know when we’ve fixed things.\n",
    "\n",
    " #### Make It Fail Every Time\n",
    "\n",
    "We can only debug something when it fails, so the second step is always to find a test case that makes it fail every time. \n",
    "\n",
    "#### Change One Thing at a Time, For a Reason\n",
    "\n",
    "Replacing random chunks of code is unlikely to do much good.\n",
    "\n",
    "#### Version Control Revisited\n",
    "\n",
    "Version control is often used to reset software to a known state during debugging, and to explore recent changes to code that might be responsible for bugs. \n",
    "\n",
    "#### Be Humble\n",
    "\n",
    "And speaking of help: if we can’t find a bug in 10 minutes, we should be humble and ask for help. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2- Debugging Basics: Exceptions, Errors, and Tracebacks\n",
    "\n",
    "When your code errors, Python will stop and return an *exception* that attempts \n",
    "to tell you what's up. There are approximately 165 exceptions in the Python standard \n",
    "library, and you'll be seeing many of them very soon. Exceptions to know\n",
    "include:\n",
    "\n",
    "    SyntaxError # You're probably missing a parenthesis or colon\n",
    "    NameError   # There's probably a variable name typo somewhere\n",
    "    TypeError   # You're doing something with incompatible variable types\n",
    "    ValueError  # You're calling a function with the wrong parameter\n",
    "    IOError     # You're trying to use a file that doesn't exist\n",
    "    IndexError  # You're trying to reference a list element that doesn't exist\n",
    "    KeyError    # Similar to an IndexError, but for dictionaries\n",
    "    Exception   # This means \"an error of any type\" - hopefully you don't see it often\n",
    "\n",
    "When code returns an exception, we say that the exception was **thrown** or\n",
    "**raised**. These exceptions may be **handled** or **caught** by the code. Speaking\n",
    "of, you can handle exceptions in Python like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(x):\n",
    "    num= sum(x)\n",
    "    den = len(x)\n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=[2, 0, 3, 4,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests could be implemented as runtime *exceptions in the function*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean(x):\n",
    "    res=-9999999999999\n",
    "    try:\n",
    "        num = sum(x)\n",
    "        den= len(x)\n",
    "        res=num/den\n",
    "    except TypeError:\n",
    "        print(\"It is not a list of numbers.\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=[2, '0', 1]\n",
    "mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes tests are functions alongside the function definitions they are testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(x):\n",
    "    res=-9999999999999\n",
    "    try:\n",
    "        num = sum(x)\n",
    "        den= len(x)\n",
    "        res=num/den\n",
    "    except TypeError:           \n",
    "        raise(\"It is not a list of numbers.\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    assert mean([0, 0, 0, 0]) == 0\n",
    "    assert mean([0, 200]) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example.** Propose a function to test the following function that returns the maximum of a list x of float numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and debug your function in case  of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raise an exception in testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age = input() \n",
    "try:\n",
    "    age = int(age)\n",
    "    if age<0:\n",
    "        raise ValueError(\"age is negative\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age = input(\"Read a strictly positive value\")\n",
    "try:\n",
    "    age = int(age)\n",
    "    assert age > 0\n",
    "except ValueError:\n",
    "    print(\"No numerical value\")\n",
    "except AssertionError:\n",
    "    print(\"year less to zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.3.  Python Testing Frameworks\n",
    "\n",
    "#### A) Numpy \n",
    "\n",
    "Numpy offers testing functions for arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.testing as npt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npt.assert_almost_equal([2,2,3], [2,2.,3.000000000001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npt.assert_almost_equal([2,2,3.1], [2,2.,3.000000000001], decimal=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npt.assert_equal([2,2,3], [2,2.,3.000000000001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Nose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write a nose test, we make assertions.\n",
    "\n",
    "    assert should_be_true()\n",
    "    assert not should_not_be_true()\n",
    "\n",
    "Additionally, nose itself defines number of assert functions which can\n",
    "be used to test more specific aspects of the code base.\n",
    "\n",
    "    from nose.tools import *\n",
    "\n",
    "    assert_equal(a, b)\n",
    "    assert_almost_equal(a, b)\n",
    "    assert_true(a)\n",
    "    assert_false(a)\n",
    "    assert_raises(exception, func, *args, **kwargs)\n",
    "    assert_is_instance(a, b)\n",
    "    # and many more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**:\n",
    "\n",
    "    $ nosetests std_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_almost_equal\n",
    "def std(vals):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test1():\n",
    "    obs = std([0.0, 2.0])\n",
    "    exp = 1.0\n",
    "    assert_equal(obs, exp)\n",
    "    \n",
    "def test2():\n",
    "    obs = std([])\n",
    "    exp = 0.0\n",
    "    assert_equal(obs, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "#### 1. Writing tests for mean()\n",
    "\n",
    "There are a few tests for the mean() function that we listed in this\n",
    "lesson. What are some tests that should fail? Add at least three test\n",
    "cases to this set. Edit the `mean_test.py` file which tests the mean()\n",
    "function in `mean.py`.\n",
    "\n",
    "*Hint:* Think about what form your input could take and what you should\n",
    "do to handle it. Also, think about the type of the elements in the list.\n",
    "What should be done if you pass a list of integers? What if you pass a\n",
    "list of strings?\n",
    "Testing Averages\n",
    "\n",
    "#### 2. Writing tests for avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg(line):\n",
    "    values = line.split(',')\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for value in values:\n",
    "        total += int(value)\n",
    "        count += 1\n",
    "    return total / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of a set of experiments are stored in a file, where the _i-th_ line\n",
    "stores the results of the _i-th_ experiment as a comma-separated list of\n",
    "integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Write Nose test cases for avg function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b=0.0\n",
    "try:\n",
    "    a = 1.0 / b\n",
    "except ZeroDivisionError:\n",
    "    print (\"Going from zero to hero.\")\n",
    "    a = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.3- Python librairies for debugging\n",
    "\n",
    "You can debug by simply attempting to run your code. This,\n",
    "however, is very annoying. First off, the code will always stop at the first exception. This means that, if you have ten errors, you'll have to run the code ten times to find all of them. Now, imagine that this is long-running code.\n",
    "Imagine waiting five minutes for your code to run, only to discover it breaks because of a typo. Doesn't that sound terrible?\n",
    "\n",
    "#### A) Pyflakes\n",
    "\n",
    "A simple program which checks Python source files for errors.  Pyflakes analyzes programs and detects various errors. It works by parsing the source file, not importing it, so it is safe to use on modules with side effects. It’s also much faster.\n",
    "\n",
    "You can run pyflakes on your code by typing:\n",
    "\n",
    "    $ pyflakes yourcode.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Pep8\n",
    "\n",
    "Pep8 checks style guide for Python code; it shows you where your code deviates from the PEP8 standard,\n",
    "\n",
    "    $ pep8 yourcode.py\n",
    "\n",
    "**Code layout**\n",
    "\n",
    "* Indentation \n",
    "* Tabs or Spaces\n",
    "* Maximum Line Length: each line has a maximum of 79 characters\n",
    "* Blank Lines \n",
    "* Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C)Autopep8\n",
    "\n",
    "`autopep8` tries to fix all of your errors for you.\n",
    "\n",
    "       $ autopep8 yourcode.py > yournewcode.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
